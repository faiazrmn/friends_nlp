---
title: "F.R.I.E.N.D.S. Text Analysis"
author: "FRK"
date: "8/15/2021"
output: 
  html_document:
    toc: TRUE
    toc_float: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, fig.height = 7, fig.width = 10)
```

```{r, echo=FALSE}
# Define variable containing url
url <- "https://upload.wikimedia.org/wikipedia/commons/b/bc/Friends_logo.svg"
```

<center><img src="`r url`"></center>

# Data Source

Data is collected from [Kaggle](https://www.kaggle.com/blessondensil294/friends-tv-series-screenplay-script). The data is in *messy* text script. Needs a lot of cleaning to make it tidy. Also scripts are separated in txt files episode wise. Need to read all scripts first.

# Data Load

We us *purrr* package's *map_dfr*, which is similar to *bind_row*.

```{r}
require(tidyverse)
require(wordcloud)
require(tidytext)

txt_files <- fs::dir_ls("C:/Users/User/Desktop/Friends NLP/", regexp = "\\.txt$")
txt_files %>% head() # List of all txt files in the folder

dat <- txt_files %>% # map_dfr reads files and bind rows, tsv read tab separated values file
  map_dfr(~read_tsv(. ,  col_names = FALSE, skip_empty_rows = T), .id = "source")

```


# Cleaning Data using strings, regexp, joining, merging, trial-error, number formatting...

```{r}
# Cleaning the Episode Names & Season number
dat_clean <- dat %>% mutate( episode =  str_split_fixed(source, " ", n = 3), # Separate Folder name, season, episode name by space
                             Season = str_remove(episode[, 2], "^NLP/"), # Delete Folder Name
                             Season = str_remove(Season, "^\\d-S\\d\\d"), # \\d detects a number, so this step removes 
                             Season = str_remove(Season, "^\\d-s\\d\\d"), # For a special case
                             Season = str_remove(Season, "E\\d\\d*"), # remove episode part
                             Season = str_remove(Season, "-S\\d\\dE\\d\\d*"), # For two episodes combined s1e1-s1e2, remove the last part
                             Season = str_remove(Season, "^S|^s"), # remove S or s to keep the season number only
                             Episode = str_remove(episode[ , 2], "^NLP/S\\d\\dE|^NLP/s\\d\\dE"), # Remove all parts except the episode number
                             Episode = str_remove(Episode, "-S\\d\\dE\\d\\d*"), # for two episodes combined s1e1-s1e2, remove the last part
                             Episode_Name = episode[, 3], # Episode name from file name
                             Episode_Name = str_remove(Episode_Name, ".txt*"), # Remove .txt part from Episode names
                             Episode_Name = str_trim(Episode_Name) # Remove white space if any, from end and start 
                             ) %>%   
  
  # Cleaning the Dialouges
  # Remove Numbered Names in Episodes in First Line
  mutate(X1 = str_remove(X1, "^[0-9]+ - [0-9]+ - |^[0-9]+ - |^[0-9]+-[0-9]+ - |^[0-9]+- |^[0-9]+|^[0-9]+ - ")) %>% 
  # 915 - 916 - The one in Barbados, 123 - , 1024-1023 - , 291-
  
  # Special case of TOW phoebes rat
  mutate(X1 = str_replace(X1, "^TOW ", "The One With ")) %>%
  
  # Remove lines Containing other than dialogues
  filter(!str_detect(X1, "closing credit|Closing Credit|^commercial break|^Commercial Break|^Part|^Dedicated|^NOTE|^Written|^Directed|^Hosted|^Produced|^Teleplay|^Friends|^FRIENDS|^opening|^Opening|^The One|^The one|^The Last|^ The One|THE ONE|^End|^The|^Originally|^Final|^Opening|^Story|^part|^Part|^Transcribed|^Lisa|^Matt|^Jennifer|OPENING")) %>% 
  
  # Remove lines starting with ( or [ or { or <
  filter(!str_detect(X1, "^\\(|^\\{+|^\\[+|\\<+")) %>% 
  
  # Get everything to lower cases
  mutate(X1 = str_to_lower(X1)) %>% 
  
  # Get Character & dialogue by splitting in two parts with ":"
  mutate(`Character&Dialogue` = str_split_fixed(X1, ":", n = 2)) %>% 
  mutate(Character = `Character&Dialogue`[, 1],
         Dialogue = `Character&Dialogue`[ , 2],
         Dialogue = str_trim(Dialogue, side = "both")) %>%  # Remove White Spaces in each line
  
  # Filter out special episode
  filter(Season != 07 & Episode != 24) %>% 
  mutate(Character = if_else(Character == "rach", "rachel", Character)) %>% 
  
  # Remove Unnecessary Columns
  select(-source, -X1, -episode, -`Character&Dialogue`) 
```

# Clean Tidy Data

```{r}
dat_clean %>% head()

# Final Check of the clean data
# Filter First row of each episode
dat_clean %>%
  group_by(Season, Episode) %>%
  filter(row_number() == 1 | row_number() == n()) %>% 
  select(Dialogue) %>% head()
```

Looks all okay to me.

# Text Analysis

## Only The main 6 character dialogues are important

```{r}
dat_clean %>% 
  count(Character, sort = T) %>% 
  head(n = 25) %>%
  ggplot(aes(n, reorder(Character, n))) +
  geom_col() +
  theme_minimal()
```


## Dialogues Per Character

```{r}
dat_clean %>% 
  filter(Character %in% c("rachel", "joey", "phoebe", "ross", "monica", "chandler")) %>% 
  group_by(Character, Season) %>% 
  summarise(n()) %>%
  left_join(dat_clean %>% 
              filter(Character %in% c("rachel", "joey", "phoebe", "ross", "monica", "chandler")) %>% 
              group_by(Character) %>% 
              summarise(sn = n()), by = "Character") %>% 
  ggplot(aes(x = reorder(Character,`n()`), y = `n()`, fill = Season)) + 
  geom_bar(stat="identity") +
  geom_text(aes(label = `n()`, group = Season), position = position_stack(vjust = .5), size = 3.5) +
  geom_text(aes(Character, sn, label = sn), nudge_y = 500) +
  labs(x = "Character", y = "Dialogues") +
  theme_minimal()
```

Rachel talks a lot. Phoebe is the most quite among the six.

## Dialogue Per Season

```{r}
dat_clean %>% 
  filter(Character %in% c("rachel", "joey", "phoebe", "ross", "monica", "chandler")) %>% 
  group_by(Season) %>% 
  summarise(n()) %>% 
  ggplot(aes(x = reorder(Season,`n()`), y = `n()`, fill = Season)) + geom_bar(stat="identity")

```

## Word Analysis

### Unnest words

```{r}
### unnest Tokens

### COnvert Dialogues into words
dat_word <- dat_clean %>%
  unnest_tokens(Word, Dialogue, drop = T)

dat_word %>% head()
```

The Word column contains individual words from each dialogue.

### Most common 30 Words by character

```{r}
dat_word %>% 
  filter(Character %in% c("rachel", "joey", "phoebe", "ross", "monica", "chandler")) %>% 
  count(Character, Word, sort = T) %>% 
  group_by(Character) %>% 
  top_n(30) %>% 
  ggplot(aes(n, reorder_within(Word, n, Character), fill = Character)) +
  geom_col(show.legend = F) +
  scale_y_reordered() +
  facet_wrap(~Character, scales = "free") +
  theme_minimal()

```


They all have similar common words. I , you, Yeah, know etc.

Lets remove the stop_words and try again.

### Most common words by character Without Stopwords

```{r}
dat_word %>% 
  anti_join(stop_words, by = c("Word" = "word")) %>%  ### Remove stopwords
  filter(Character %in% c("rachel", "joey", "phoebe", "ross", "monica", "chandler")) %>% 
  count(Character, Word, sort = T) %>% 
  group_by(Character) %>% 
  top_n(30) %>% 
  ggplot(aes(n, reorder_within(Word, n, Character), fill = Character)) +
  geom_col(show.legend = F) +
  scale_y_reordered() +
  facet_wrap(~Character, scales = "free") +
  theme_minimal()
```



Still kinda looks similar. But the number of times they call each other is interesting.


### Word Frequency and Histogram

```{r}
dat_word %>% 
  anti_join(stop_words, by = c("Word" = "word")) %>% 
  filter(Character %in% c("rachel", "joey", "phoebe", "ross", "monica", "chandler")) %>% 
  count(Character, Word, sort = T) %>% 
  group_by(Character) %>% 
  add_count(Total = sum(n)) %>% 
  
  ggplot(aes(n/Total, fill = Character)) +
  geom_histogram(show.legend = FALSE) +
  xlim(NA, 0.0005) +
  facet_wrap(~Character, ncol = 3, scales = "free_y")
```

As expected. Most common words appear most of the times. It's the words at the right tail that separates the characters from each other.

### Wordcloud for Monica
```{r}
dat_word %>% 
  anti_join(stop_words, by = c("Word" = "word")) %>% 
  filter(Character == "monica") %>% 
  count(Word, sort = T) %>%
  with(wordcloud(words = Word, freq = n, random.order = T, max.words = 100 ))

```

Bathroom, alright, guys, baby, rach... sound pretty Monica to me !

### Wordcloud for Phoebe
```{r}
dat_word %>% 
  anti_join(stop_words, by = c("Word" = "word")) %>% 
  filter(Character == "phoebe") %>% 
  count(Word, sort = T) %>%
  with(wordcloud(words = Word, freq = n, random.order = T, max.words = 100 ))

```

Cat, hey, singing, remember, massage, totally... that's Phoebe !!!

### Ross & Rachel Common Words

```{r}
library(scales)

dat_word %>% 
  anti_join(stop_words, by = c("Word" = "word")) %>% 
  filter(Character %in% c("rachel", "ross")) %>% 
  mutate(Word = str_extract(Word, "[a-z']+")) %>% 
  count(Character, Word) %>%
  group_by(Character) %>% 
  mutate(proportion = round(n / sum(n), 100)) %>% 
  select(-n) %>% 
  pivot_wider(names_from = Character, values_from = proportion) %>% 
  
  ggplot(aes(x = rachel, y = ross, color = abs(rachel - ross) )) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.05, size = 1, width = 0.1, height = 0.1) +
  geom_text(aes(x = rachel, y = ross, label = Word), check_overlap = T, vjust = 0.5, size = 3.5) + 
  scale_x_log10() +
  scale_y_log10() +
  scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
  theme_minimal() +
  theme(legend.position="none")
```

Words above the line are more used by Ross, and words below by Rach !

### Chandler and Monica Words
```{r}
dat_word %>% 
  anti_join(stop_words, by = c("Word" = "word")) %>% 
  filter(Character %in% c("chandler", "monica")) %>% 
  mutate(Word = str_extract(Word, "[a-z']+")) %>% 
  count(Character, Word) %>%
  group_by(Character) %>% 
  mutate(proportion = round(n / sum(n), 100)) %>% 
  select(-n) %>% 
  pivot_wider(names_from = Character, values_from = proportion) %>% 
  
  ggplot(aes(x = chandler, y = monica, color = abs(chandler - monica) )) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.05, size = 1, width = 0.1, height = 0.1) +
  geom_text(aes(x = chandler, y = monica, label = Word), check_overlap = T, vjust = 0.5, size = 3.5) + 
  scale_x_log10() +
  scale_y_log10() +
  scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
  theme_minimal() +
  theme(legend.position="none") 

```

### Joey and Pheeebs Words

```{r}
dat_word %>% 
  anti_join(stop_words, by = c("Word" = "word")) %>% 
  filter(Character %in% c("joey", "phoebe")) %>% 
  mutate(Word = str_extract(Word, "[a-z']+")) %>% 
  count(Character, Word) %>%
  group_by(Character) %>% 
  mutate(proportion = round(n / sum(n), 100)) %>% 
  select(-n) %>% 
  pivot_wider(names_from = Character, values_from = proportion) %>% 
  
  ggplot(aes(x = joey, y = phoebe, color = abs(joey - phoebe) )) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.05, size = 1, width = 0.1, height = 0.1) +
  geom_text(aes(x = joey, y = phoebe, label = Word), check_overlap = T, vjust = 0.5, size = 3.5) + 
  scale_x_log10() +
  scale_y_log10() +
  scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
  theme_minimal() +
  theme(legend.position="none")
```

### TF-IDF (Term Frequency and Inverse Document Frequency)

This method extracts the word only exclusive to each character.

```{r}
dat_tf <- dat_clean %>% 
  filter(Character %in% c("rachel", "joey", "phoebe", "ross", "monica", "chandler")) %>% 
  unnest_tokens(Word, Dialogue, drop = F) %>%
  anti_join(stop_words, by = c("Word" = "word")) %>% 
  count(Character, Word, sort = TRUE)

dat_tf %>% head()

dat_tfidf <- dat_tf %>% 
  bind_tf_idf( term =  Word, document = Character, n = n) %>% 
  arrange(desc(tf_idf))

dat_tfidf %>% head()

dat_tfidf %>% 
  group_by(Character) %>% 
  slice_max(tf_idf, n = 30) %>% 
  ggplot(aes(tf_idf, reorder_within(Word, tf_idf, Character), fill = Character)) +
  geom_col(show.legend = F) +
  scale_y_reordered() +
  facet_wrap(~Character, scales = "free") +
  theme_minimal()
```


```{r, echo=FALSE}
# Define variable containing url
url <- "https://upload.wikimedia.org/wikipedia/en/d/d6/Friends_season_one_cast.jpg"
```

<center><img src="`r url`"></center>